var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = ODERNNDynamics","category":"page"},{"location":"#ODERNNDynamics","page":"Home","title":"ODERNNDynamics","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for ODERNNDynamics.","category":"page"},{"location":"","page":"Home","title":"Home","text":"NNDynamics.jl learns a model of the environment. It works with the following environments:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pendulum\nAcrobot\nLunar Lander","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In the julia REPL, run","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(url=\"https://github.com/SvenDuve/ODERNNDynamics.jl\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is not for general use. To be fully functional, it requires the following packages:","category":"page"},{"location":"","page":"Home","title":"Home","text":"RLTypes\nConda\nPyCall\nGymnasium, this is a slightly adapted version. The original package can be found here","category":"page"},{"location":"","page":"Home","title":"Home","text":"Run the following:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(url=\"https://github.com/SvenDuve/RLTypes.jl\")\nPkg.add(\"Conda\")\nPkg.add(\"PyCall\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Then again within julia,","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Conda\nusing PyCall\nConda.pip_interop(true)\nConda.add(\"wheel\")\nConda.add(\"box2d-py\")\nConda.add(\"pygame\")\nConda.pip(\"install\", \"git+https://github.com/SvenDuve/Gymnasium\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Bring package into scope with","category":"page"},{"location":"","page":"Home","title":"Home","text":"using ODERNNDynamics","category":"page"},{"location":"#Example-usage","page":"Home","title":"Example usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Once the above is installed, the following code can be used to train an agent on data collected form 100 episodes of the Pendulum environment:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using ODERNNDynamics\nusing RLTypes\nmodel(Acrobot(), ModelParameter(collect_train=100, collect_test=10))","category":"page"},{"location":"","page":"Home","title":"Home","text":"For further parameters, see the code.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This function returns a data structure containing a trained model. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [ODERNNDynamics]","category":"page"},{"location":"#ODERNNDynamics.ODE_RNN-Tuple{Any, Any, Any}","page":"Home","title":"ODERNNDynamics.ODE_RNN","text":"(m::ODE_RNN)(timestamps, datapoints, hidden)\n\nReturns the hidden state of the ODE_RNN model at the given timestamps and datapoints.\n\n\n\n\n\n","category":"method"},{"location":"#ODERNNDynamics.ODE_RNN-Tuple{Int64, Int64}","page":"Home","title":"ODERNNDynamics.ODE_RNN","text":"ODE_RNN(input_size::Int, hidden_size::Int)\n\nConstructs an ODE_RNN model with the given input and hidden size.\n\n\n\n\n\n","category":"method"},{"location":"#ODERNNDynamics.accuracy-Tuple{Any, Any, Any}","page":"Home","title":"ODERNNDynamics.accuracy","text":"accuracy(y_true, y_pred, tolerance)\n\nReturns the accuracy of the prediction ypred compared to the true value ytrue.\n\n\n\n\n\n","category":"method"},{"location":"#ODERNNDynamics.modelEnv-Tuple{RLTypes.ContinuousEnvironment, RLTypes.ModelParameter}","page":"Home","title":"ODERNNDynamics.modelEnv","text":"modelEnv(environment::ContinuousEnvironment, modelParams::ModelParameter)\n\nMain algorithm to train a continuous action model.\n\n\n\n\n\n","category":"method"},{"location":"#ODERNNDynamics.modelEnv-Tuple{RLTypes.DiscreteEnvironment, RLTypes.ModelParameter}","page":"Home","title":"ODERNNDynamics.modelEnv","text":"modelEnv(environment::DiscreteEnvironment, modelParams::ModelParameter)\n\nMain algorithm to train a discrete action model.\n\n\n\n\n\n","category":"method"},{"location":"#ODERNNDynamics.train_step!-Tuple{RLTypes.ContinuousEnvironment, Vararg{Any, 9}}","page":"Home","title":"ODERNNDynamics.train_step!","text":"train_step!(environment::ContinuousEnvironment, S, A, R, S´, T, hidden, timestamps, fθ, model_opt)\n\nPerforms a training step on the given continuous action model with gradient descent.\n\n\n\n\n\n","category":"method"},{"location":"#ODERNNDynamics.train_step!-Tuple{RLTypes.DiscreteEnvironment, Any, Any, Any, Any, Any, Any, Any, Any, Any, RLTypes.EnvParameter}","page":"Home","title":"ODERNNDynamics.train_step!","text":"train_step!(environment::DiscreteEnvironment, S, A, R, S´, T, hidden, timestamps, fθ, model_opt, ep::EnvParameter)\n\nPerforms a training step on the given discrete action model with gradient descent.\n\n\n\n\n\n","category":"method"}]
}
